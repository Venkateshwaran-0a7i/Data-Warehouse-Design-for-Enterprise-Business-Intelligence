# Data Warehouse Design and ETL Optimization for Enterprise Business Intelligence

## ğŸ“Œ Project Overview
This final-year project designs and implements a **centralized Data Warehouse (DWH)** with an **optimized ETL pipeline** to support **Enterprise Business Intelligence (BI)**.  
The system integrates data from multiple operational sources, cleans and transforms it using **Python**, loads it into **Microsoft SQL Server** using a **Star Schema**, and visualizes insights through **Power BI dashboards**.

---

## ğŸ¯ Project Title
**Data Warehouse Design and ETL Optimization for Enterprise Business Intelligence**

---

## ğŸ§  Problem Statement
Operational databases (OLTP) are optimized for transaction processing, not analytics. Running complex analytical queries on OLTP systems leads to:
- Slow query performance
- Data silos and inconsistent reporting
- Limited historical trend analysis
- Manual, time-consuming reporting processes

This project solves the problem by building an **Enterprise Data Warehouse** with an optimized ETL pipeline to enable fast, accurate, and scalable analytics.

---

## âœ… Objectives
- Clean and prepare raw business data for analysis
- Design a **dimensional model (Star Schema)** in SQL Server
- Implement an end-to-end **Python ETL pipeline**
- Optimize ETL and query performance using indexing and incremental loads
- Build interactive **Power BI dashboards** for KPI reporting and insights

---


## ğŸ—ï¸ System Architecture

**Flow:**

Operational Data Sources â†’ Staging Layer â†’ ETL Pipeline â†’ Data Warehouse â†’ BI Dashboards

### Key Components

* **Source Systems:** Sales, HR, Finance (CSV / SQL)
* â†“
* **Staging Layer:** Raw data validation and cleansing
* â†“
* **ETL Layer:** Incremental extraction, transformation, aggregation
* â†“
* **Data Warehouse:** Fact and Dimension tables
* â†“
* **BI Layer:** KPI dashboards and analytical reports

---

## ğŸ§© Data Warehouse Design (Star Schema)

### â­ Fact Table
**FactSales**
- Stores transactional measures like:
  - `Quantity`
  - `UnitPrice`
  - `TotalAmount`
  - `Profit` (optional)

### ğŸ“Œ Dimension Tables
- **DimCustomers** (customer profile, purchase behavior)
- **DimProducts** (product category, description)
- **DimTime** (day, month, year, quarter)
- **DimRegion** (optional)

---

## ğŸ”„ ETL Pipeline Workflow (Python)

### 1ï¸âƒ£ Extract
- Read data from CSV / Excel / SQL sources
- Store raw data into **staging tables**

### 2ï¸âƒ£ Transform
- Remove duplicates
- Handle missing values
- Standardize date/time formats
- Validate values (example: negative quantity checks)
- Generate surrogate keys (if required)

### 3ï¸âƒ£ Load
- Load dimension tables first
- Load fact tables last (FK dependency)
- Maintain referential integrity

---

## âš¡ ETL Optimization Techniques
To make the project advanced-level, the following optimizations are applied:
- Incremental loading (timestamp/ID-based)
- Indexing on foreign keys in fact tables
- Pre-aggregation for common KPI reports
- Partitioning fact tables by date (optional)
- Removing redundant transformations

---

## ğŸ“Š BI Dashboard (Power BI)
Power BI reports include:
- Total Sales / Profit / Orders
- Monthly & yearly sales trends
- Top products and customers
- Region-wise performance
- Sales funnel / conversion insights (optional)

---

## ğŸ› ï¸ Technology Stack

### Software Requirements
- Python 3.x (Pandas, SQLAlchemy, pyodbc)
- Microsoft SQL Server (Developer / Express)
- Power BI Desktop
- Git & GitHub

### Hardware Requirements
- CPU: x64 â‰¥ 2.0 GHz
- RAM: 8 GB recommended
- Storage: 100 GB SSD recommended

---


## ğŸ› ï¸ Technology Stack

| Layer              | Tools                |
| ------------------ | -------------------- |
| Data Sources       | CSV, SQL             |
| ETL                | Python (Pandas), SQL |
| Data Warehouse     | PostgreSQL / MySQL   |
| BI & Visualization | Power BI / Tableau   |
| Version Control    | Git, GitHub          |

---

## ğŸ“ Project Structure

```
data-warehouse-etl-bi/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # Original source files (never edit)
â”‚   â”œâ”€â”€ staging/              # Cleaned/intermediate outputs (optional)
â”‚   â””â”€â”€ processed/            # Final processed datasets (optional)
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ 01_create_database.sql
â”‚   â”œâ”€â”€ 02_create_staging_tables.sql
â”‚   â”œâ”€â”€ 03_create_dimensions.sql
â”‚   â”œâ”€â”€ 04_create_fact_tables.sql
â”‚   â”œâ”€â”€ 05_indexes_partitions.sql
â”‚   â””â”€â”€ 06_views_for_powerbi.sql
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ db_config.env      # DB connection variables (DON'T push real secrets)
â”‚   â”‚
â”‚   â”œâ”€â”€ extract.py             # Extract from CSV/SQL sources
â”‚   â”œâ”€â”€ transform.py           # Cleaning + transformations
â”‚   â”œâ”€â”€ load_staging.py        # Load into staging tables
â”‚   â”œâ”€â”€ load_warehouse.py      # Load into dim + fact tables
â”‚   â”œâ”€â”€ incremental_load.py    # CDC / timestamp-based incremental logic
â”‚   â””â”€â”€ run_etl.py             # Main pipeline runner
â”‚
â”œâ”€â”€ notebooks/                 # Optional (EDA + testing)
â”‚   â”œâ”€â”€ data_profiling.ipynb
â”‚   â””â”€â”€ etl_testing.ipynb
â”‚
â”œâ”€â”€ dashboards/
â”‚   â”œâ”€â”€ powerbi_dashboard.pbix
â”‚   â””â”€â”€ screenshots/
â”‚       â”œâ”€â”€ dashboard_page1.png
â”‚       â””â”€â”€ dashboard_page2.png
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture_diagram.png
â”‚   â”œâ”€â”€ star_schema.png
â”‚   â”œâ”€â”€ project_report.pdf
â”‚   â””â”€â”€ ppt/
â”‚       â””â”€â”€ final_presentation.pptx
â”‚
â””â”€â”€ evaluation/
    â”œâ”€â”€ etl_runtime_results.csv
    â”œâ”€â”€ query_performance_results.csv
    â””â”€â”€ performance_summary.md

```

---

## ğŸ“„ Abstract

Enterprises generate large volumes of data across multiple operational systems, which are not optimized for analytical workloads. This project presents the design and implementation of an optimized data warehouse framework to support enterprise Business Intelligence. A structured ETL pipeline is developed to integrate, cleanse, and transform heterogeneous data sources using dimensional modeling techniques. ETL optimization strategies such as incremental loading, indexing, and partitioning are applied to enhance performance and scalability. Experimental evaluation shows improved query response time and reporting efficiency compared to traditional operational databases, providing a reliable foundation for data-driven decision-making.

---

## ğŸ“ˆ Evaluation Metrics
The system is evaluated using:
- ETL execution time (before vs after optimization)
- Query response time (OLTP vs DWH)
- Data consistency and accuracy
- Reporting efficiency

---

## ğŸ“š References
- Ralph Kimball & Margy Ross â€” *The Data Warehouse Toolkit*
- W.H. Inmon â€” *Building the Data Warehouse*
- Microsoft Documentation â€” SQL Server & Power BI
- IEEE Base Paper (Enterprise ETL + Data Warehousing)

---

## ğŸ‘¤ Author
**Venkateshwaran Mani**  
B.Tech â€“ Artificial Intelligence & Data Science  


---

## â­ Note
This project demonstrates real-world skills in:
- Data Warehousing
- ETL Engineering
- SQL Server Analytics
- Power BI Reporting
- Performance Optimization

If you found this useful, give the repo a â­
