
# Data Warehouse Design and ETL Optimization for Enterprise Business Intelligence

<p align="left">
  <b>Enterprise Data Warehouse (SQL Server) + Optimized Python ETL + BI Reporting</b>
</p>

---

## ğŸ“Œ Project Overview
This final-year project designs and implements a **centralized Data Warehouse (DWH)** with an **optimized ETL pipeline** to support **Enterprise Business Intelligence (BI)**.  
The system integrates data from operational sources, cleans and transforms it using **Python**, loads it into **Microsoft SQL Server** using a **Star Schema**, and enables analytics using **Power BI dashboards**.

---

## ğŸ¯ Project Title
**Data Warehouse Design and ETL Optimization for Enterprise Business Intelligence**

---

## ğŸ§  Problem Statement
Operational databases (**OLTP**) are designed for transaction processing, not analytics. Running complex analytical queries directly on OLTP systems leads to:
- Slow query performance
- Data silos and inconsistent reports
- Limited historical analysis
- Manual and time-consuming reporting workflows

This project solves these issues by building a **Data Warehouse** and an optimized **ETL pipeline** for fast and reliable analytics.

---

## âœ… Objectives
- Clean and prepare raw business data for analysis
- Design a **Star Schema** (Fact + Dimension tables) in SQL Server
- Implement an end-to-end **Python ETL pipeline**
- Optimize ETL + query performance using **indexing and incremental loading**
- Build interactive **Power BI dashboards** for KPI reporting

---

## ğŸ—ï¸ System Architecture
**Flow:**
```

Operational Data Sources (CSV / SQL)
â†“
Staging Layer (SQL Server)
â†“
Python ETL (Extract â†’ Transform â†’ Load)
â†“
Enterprise Data Warehouse (Star Schema)
â†“
Power BI Dashboards (KPIs & Analytics)

```

---

## ğŸ§© Data Warehouse Design (Star Schema)

### â­ Fact Table
**FactSales**
- Measures:
  - `Quantity`
  - `UnitPrice`
  - `TotalAmount`
  - `Profit` (optional)

### ğŸ“Œ Dimension Tables
- **DimCustomers** (customer profile)
- **DimProducts** (product details)
- **DimTime** (day/month/year)
- **DimRegion** (optional)

---

## ğŸ”„ ETL Pipeline Workflow (Python)

### 1ï¸âƒ£ Extract
- Reads data from CSV / SQL sources
- Loads raw data into staging tables

### 2ï¸âƒ£ Transform
- Removes duplicates
- Handles missing values
- Standardizes formats (dates, numeric fields)
- Validates business rules (negative quantity, invalid price, etc.)

### 3ï¸âƒ£ Load
- Loads dimension tables first
- Loads fact table last (FK dependency)
- Maintains referential integrity

---

## âš¡ ETL Optimization Techniques
To make this project advanced-level, the following optimizations are applied:
- Incremental loading (timestamp/ID-based)
- Indexing on fact table foreign keys
- Pre-aggregation for common KPIs
- Partitioning by date (optional)
- Removing redundant transformations

---

## ğŸ“Š BI Dashboard (Power BI)
Dashboards include:
- Total Sales / Orders / Profit
- Monthly and yearly sales trends
- Top products and customers
- Region-wise performance
- Drill-down and filtering

---

## ğŸ› ï¸ Technology Stack

<p align="left"><b>Core Stack</b></p>
<div align="left">
  <img src="https://skillicons.dev/icons?i=python" height="40" alt="python" />
  <img width="10" />
  <img src="https://skillicons.dev/icons?i=github" height="40" alt="github" />
  <img width="10" />
  <img src="https://skillicons.dev/icons?i=git" height="40" alt="git" />
  <img width="10" />
  <img src="https://skillicons.dev/icons?i=windows" height="40" alt="windows" />
</div>

<br/>

<p align="left"><b>Database & BI</b></p>
<div align="left">
  <img src="https://skillicons.dev/icons?i=mssql" height="40" alt="sqlserver" />
  <img width="10" />
  <img src="https://raw.githubusercontent.com/microsoft/PowerBI-Icons/main/SVG/Power-BI.svg" height="40" alt="powerbi" />
</div>

<br/>

<p align="left"><b>Python Libraries</b></p>
<div align="left">
  <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/pandas/pandas-original.svg" height="40" alt="pandas" />
  <img width="10" />
  <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/numpy/numpy-original.svg" height="40" alt="numpy" />
</div>

---

## ğŸ“ Project Structure (Current Repo)
```

data-warehouse-etl-bi/
â”‚
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â”œâ”€â”€ 20100072.csv
â”‚       â”œâ”€â”€ 20100072_MetaData.csv
â”‚       â””â”€â”€ warehouse.db                # optional local testing file
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract.py                      # extract data from raw source
â”‚   â”œâ”€â”€ transform.py                    # cleaning + transformations
â”‚   â”œâ”€â”€ load_staging.py                 # load into staging tables
â”‚   â”œâ”€â”€ load_warehouse.py               # load into dim + fact tables
â”‚   â””â”€â”€ run_etl.py                      # runs complete ETL pipeline
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ 01_create_database.sql
â”‚   â”œâ”€â”€ 02_create_staging_tables.sql
â”‚   â”œâ”€â”€ 03_create_dimensions.sql
â”‚   â”œâ”€â”€ 04_create_fact_tables.sql
â”‚   â”œâ”€â”€ 05_indexes_partitions.sql
â”‚   â””â”€â”€ 06_views_for_powerbi.sql
â”‚
â””â”€â”€ evaluation/
â”œâ”€â”€ query_performance_results.csv
â””â”€â”€ verify_warehouse.py

````

---

## ğŸš€ How to Run the Project

### Step 1: Setup SQL Server Tables
Run SQL scripts in order:
1. `sql/01_create_database.sql`
2. `sql/02_create_staging_tables.sql`
3. `sql/03_create_dimensions.sql`
4. `sql/04_create_fact_tables.sql`
5. `sql/05_indexes_partitions.sql`
6. `sql/06_views_for_powerbi.sql`

### Step 2: Run ETL Pipeline
```bash
python etl/run_etl.py
````

---

## ğŸ“ˆ Evaluation Metrics

The system is evaluated using:

* ETL execution time (before vs after optimization)
* Query response time (OLTP vs Data Warehouse)
* Data consistency and accuracy
* Reporting efficiency

---

## ğŸ“„ Abstract

Enterprises generate large volumes of data across multiple operational systems, which are not optimized for analytical workloads. This project presents the design and implementation of an optimized data warehouse framework to support enterprise Business Intelligence. A structured ETL pipeline is developed to integrate, cleanse, and transform heterogeneous data sources using dimensional modeling techniques. ETL optimization strategies such as incremental loading, indexing, and partitioning are applied to enhance performance and scalability. Experimental evaluation shows improved query response time and reporting efficiency compared to traditional operational databases, providing a reliable foundation for data-driven decision-making.

---

## ğŸ“š References

* Ralph Kimball & Margy Ross â€” *The Data Warehouse Toolkit*
* W.H. Inmon â€” *Building the Data Warehouse*
* Microsoft Documentation â€” SQL Server & Power BI
* IEEE Base Paper â€” Enterprise ETL + Data Warehousing

---

## ğŸ‘¤ Author

**Venkateshwaran Mani**
B.Tech â€“ Artificial Intelligence & Data Science

---

## â­ Note

This project demonstrates real-world skills in:

* Data Warehousing
* ETL Engineering
* SQL Server Analytics
* Power BI Reporting
* Performance Optimization

If you found this useful, give the repo a â­
